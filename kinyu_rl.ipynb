{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class State():\n",
    "\n",
    "    def __init__(self, row=-1, column=-1):\n",
    "        self.row = row\n",
    "        self.column = column\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<State: [{}, {}]>\".format(self.row, self.column)\n",
    "\n",
    "    def clone(self):\n",
    "        return State(self.row, self.column)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.row, self.column))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.column == other.column\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 1\n",
    "    DOWN = -1\n",
    "    LEFT = 2\n",
    "    RIGHT = -2\n",
    "\n",
    "\n",
    "class Environment():\n",
    "\n",
    "    def __init__(self, grid, move_prob=0.8):\n",
    "        # grid is 2d-array. Its values are treated as an attribute.\n",
    "        # Kinds of attribute is following.\n",
    "        #  0: ordinary cell\n",
    "        #  -1: damage cell (game end)\n",
    "        #  1: reward cell (game end)\n",
    "        #  9: block cell (can't locate agent)\n",
    "        self.grid = grid\n",
    "        self.agent_state = State()\n",
    "\n",
    "        # Default reward is minus. Just like a poison swamp.\n",
    "        # It means the agent has to reach the goal fast!\n",
    "        self.default_reward = -0.04\n",
    "\n",
    "        # Agent can move to a selected direction in move_prob.\n",
    "        # It means the agent will move different direction\n",
    "        # in (1 - move_prob).\n",
    "        self.move_prob = move_prob\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def row_length(self):\n",
    "        return len(self.grid)\n",
    "\n",
    "    @property\n",
    "    def column_length(self):\n",
    "        return len(self.grid[0])\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return [Action.UP, Action.DOWN,\n",
    "                Action.LEFT, Action.RIGHT]\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        states = []\n",
    "        for row in range(self.row_length):\n",
    "            for column in range(self.column_length):\n",
    "                # Block cells are not included to the state.\n",
    "                if self.grid[row][column] != 9:\n",
    "                    states.append(State(row, column))\n",
    "        return states\n",
    "\n",
    "    def transit_func(self, state, action):\n",
    "        transition_probs = {}\n",
    "        if not self.can_action_at(state):\n",
    "            # Already on the terminal cell.\n",
    "            return transition_probs\n",
    "\n",
    "        opposite_direction = Action(action.value * -1)\n",
    "\n",
    "        for a in self.actions:\n",
    "            prob = 0\n",
    "            if a == action:\n",
    "                prob = self.move_prob\n",
    "            elif a != opposite_direction:\n",
    "                prob = (1 - self.move_prob) / 2\n",
    "\n",
    "            next_state = self._move(state, a)\n",
    "            if next_state not in transition_probs:\n",
    "                transition_probs[next_state] = prob\n",
    "            else:\n",
    "                transition_probs[next_state] += prob\n",
    "\n",
    "        return transition_probs\n",
    "\n",
    "    def can_action_at(self, state):\n",
    "        if self.grid[state.row][state.column] == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _move(self, state, action):\n",
    "        if not self.can_action_at(state):\n",
    "            raise Exception(\"Can't move from here!\")\n",
    "\n",
    "        next_state = state.clone()\n",
    "\n",
    "        # Execute an action (move).\n",
    "        if action == Action.UP:\n",
    "            next_state.row -= 1\n",
    "        elif action == Action.DOWN:\n",
    "            next_state.row += 1\n",
    "        elif action == Action.LEFT:\n",
    "            next_state.column -= 1\n",
    "        elif action == Action.RIGHT:\n",
    "            next_state.column += 1\n",
    "\n",
    "        # Check whether a state is out of the grid.\n",
    "        if not (0 <= next_state.row < self.row_length):\n",
    "            next_state = state\n",
    "        if not (0 <= next_state.column < self.column_length):\n",
    "            next_state = state\n",
    "\n",
    "        # Check whether the agent bumped a block cell.\n",
    "        if self.grid[next_state.row][next_state.column] == 9:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward_func(self, state):\n",
    "        reward = self.default_reward\n",
    "        done = False\n",
    "\n",
    "        # Check an attribute of next state.\n",
    "        attribute = self.grid[state.row][state.column]\n",
    "        if attribute == 1:\n",
    "            # Get reward! and the game ends.\n",
    "            reward = 1\n",
    "            done = True\n",
    "        elif attribute == -1:\n",
    "            # Get damage! and the game ends.\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        # Locate the agent at lower left corner.\n",
    "        self.agent_state = State(self.row_length - 1, 0)\n",
    "        return self.agent_state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, done = self.transit(self.agent_state, action)\n",
    "        if next_state is not None:\n",
    "            self.agent_state = next_state\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def transit(self, state, action):\n",
    "        transition_probs = self.transit_func(state, action)\n",
    "        if len(transition_probs) == 0:\n",
    "            return None, None, True\n",
    "\n",
    "        next_states = []\n",
    "        probs = []\n",
    "        for s in transition_probs:\n",
    "            next_states.append(s)\n",
    "            probs.append(transition_probs[s])\n",
    "\n",
    "        next_state = np.random.choice(next_states, p=probs)\n",
    "        reward, done = self.reward_func(next_state)\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Agent gets 0.31999999999999995 reward.\n",
      "Episode 1: Agent gets -1.24 reward.\n",
      "Episode 2: Agent gets -0.4400000000000006 reward.\n",
      "Episode 3: Agent gets -0.6800000000000008 reward.\n",
      "Episode 4: Agent gets -2.08 reward.\n",
      "Episode 5: Agent gets -4.480000000000002 reward.\n",
      "Episode 6: Agent gets -3.5600000000000014 reward.\n",
      "Episode 7: Agent gets -1.32 reward.\n",
      "Episode 8: Agent gets -2.2 reward.\n",
      "Episode 9: Agent gets -1.6800000000000002 reward.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#from environment import Environment\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.actions = env.actions\n",
    "\n",
    "    def policy(self, state):\n",
    "        return random.choice(self.actions)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Make grid environment.\n",
    "    grid = [\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 9, 0, -1],\n",
    "        [0, 0, 0, 0]\n",
    "    ]\n",
    "    env = Environment(grid)\n",
    "    agent = Agent(env)\n",
    "\n",
    "    # Try 10 game.\n",
    "    for i in range(10):\n",
    "        # Initialize position of agent.\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        print(\"Episode {}: Agent gets {} reward.\".format(i, total_reward))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880942034605892\n",
      "0.9068026334400001\n",
      "-0.96059601\n"
     ]
    }
   ],
   "source": [
    "def V(s, gamma=0.99):\n",
    "    V = R(s) + gamma * max_V_on_next_state(s)\n",
    "    return V\n",
    "\n",
    "\n",
    "def R(s):\n",
    "    if s == \"happy_end\":\n",
    "        return 1\n",
    "    elif s == \"bad_end\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def max_V_on_next_state(s):\n",
    "    # If game end, expected value is 0.\n",
    "    if s in [\"happy_end\", \"bad_end\"]:\n",
    "        return 0\n",
    "\n",
    "    actions = [\"up\", \"down\"]\n",
    "    values = []\n",
    "    for a in actions:\n",
    "        transition_probs = transit_func(s, a)\n",
    "        v = 0\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            v += prob * V(next_state)\n",
    "        values.append(v)\n",
    "    return max(values)\n",
    "\n",
    "\n",
    "def transit_func(s, a):\n",
    "    \"\"\"\n",
    "    Make next state by adding action str to state.\n",
    "    ex: (s = 'state', a = 'up') => 'state_up'\n",
    "        (s = 'state_up', a = 'down') => 'state_up_down'\n",
    "    \"\"\"\n",
    "\n",
    "    actions = s.split(\"_\")[1:]\n",
    "    LIMIT_GAME_COUNT = 5\n",
    "    HAPPY_END_BORDER = 4\n",
    "    MOVE_PROB = 0.9\n",
    "\n",
    "    def next_state(state, action):\n",
    "        return \"_\".join([state, action])\n",
    "\n",
    "    if len(actions) == LIMIT_GAME_COUNT:\n",
    "        up_count = sum([1 if a == \"up\" else 0 for a in actions])\n",
    "        state = \"happy_end\" if up_count >= HAPPY_END_BORDER else \"bad_end\"\n",
    "        prob = 1.0\n",
    "        return {state: prob}\n",
    "    else:\n",
    "        opposite = \"up\" if a == \"down\" else \"down\"\n",
    "        return {\n",
    "            next_state(s, a): MOVE_PROB,\n",
    "            next_state(s, opposite): 1 - MOVE_PROB\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(V(\"state\"))\n",
    "    print(V(\"state_up_up\"))\n",
    "    print(V(\"state_down_down\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Planner():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.log = []\n",
    "\n",
    "    def initialize(self):\n",
    "        self.env.reset()\n",
    "        self.log = []\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        raise Exception(\"Planner have to implements plan method.\")\n",
    "\n",
    "    def transitions_at(self, state, action):\n",
    "        transition_probs = self.env.transit_func(state, action)\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            reward, _ = self.env.reward_func(next_state)\n",
    "            yield prob, next_state, reward\n",
    "\n",
    "    def dict_to_grid(self, state_reward_dict):\n",
    "        grid = []\n",
    "        for i in range(self.env.row_length):\n",
    "            row = [0] * self.env.column_length\n",
    "            grid.append(row)\n",
    "        for s in state_reward_dict:\n",
    "            grid[s.row][s.column] = state_reward_dict[s]\n",
    "\n",
    "        return grid\n",
    "\n",
    "\n",
    "class ValuteIterationPlanner(Planner):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        self.initialize()\n",
    "        actions = self.env.actions\n",
    "        V = {}\n",
    "        for s in self.env.states:\n",
    "            # Initialize each state's expected reward.\n",
    "            V[s] = 0\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            self.log.append(self.dict_to_grid(V))\n",
    "            for s in V:\n",
    "                if not self.env.can_action_at(s):\n",
    "                    continue\n",
    "                expected_rewards = []\n",
    "                for a in actions:\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                max_reward = max(expected_rewards)\n",
    "                delta = max(delta, abs(max_reward - V[s]))\n",
    "                V[s] = max_reward\n",
    "\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        V_grid = self.dict_to_grid(V)\n",
    "        return V_grid\n",
    "\n",
    "\n",
    "class PolicyIterationPlanner(Planner):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.policy = {}\n",
    "\n",
    "    def initialize(self):\n",
    "        super().initialize()\n",
    "        self.policy = {}\n",
    "        actions = self.env.actions\n",
    "        states = self.env.states\n",
    "        for s in states:\n",
    "            self.policy[s] = {}\n",
    "            for a in actions:\n",
    "                # Initialize policy.\n",
    "                # At first, each action is taken uniformly.\n",
    "                self.policy[s][a] = 1 / len(actions)\n",
    "\n",
    "    def estimate_by_policy(self, gamma, threshold):\n",
    "        V = {}\n",
    "        for s in self.env.states:\n",
    "            # Initialize each state's expected reward.\n",
    "            V[s] = 0\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in V:\n",
    "                expected_rewards = []\n",
    "                for a in self.policy[s]:\n",
    "                    action_prob = self.policy[s][a]\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        r += action_prob * prob * \\\n",
    "                             (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                max_reward = max(expected_rewards)\n",
    "                delta = max(delta, abs(max_reward - V[s]))\n",
    "                V[s] = max_reward\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        return V\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        self.initialize()\n",
    "        states = self.env.states\n",
    "        actions = self.env.actions\n",
    "\n",
    "        def take_max_action(action_value_dict):\n",
    "            return max(action_value_dict, key=action_value_dict.get)\n",
    "\n",
    "        while True:\n",
    "            update_stable = True\n",
    "            # Estimate expected rewards under current policy.\n",
    "            V = self.estimate_by_policy(gamma, threshold)\n",
    "            self.log.append(self.dict_to_grid(V))\n",
    "\n",
    "            for s in states:\n",
    "                # Get an action following to the current policy.\n",
    "                policy_action = take_max_action(self.policy[s])\n",
    "\n",
    "                # Compare with other actions.\n",
    "                action_rewards = {}\n",
    "                for a in actions:\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    action_rewards[a] = r\n",
    "                best_action = take_max_action(action_rewards)\n",
    "                if policy_action != best_action:\n",
    "                    update_stable = False\n",
    "\n",
    "                # Update policy (set best_action prob=1, otherwise=0 (greedy))\n",
    "                for a in self.policy[s]:\n",
    "                    prob = 1 if a == best_action else 0\n",
    "                    self.policy[s][a] = prob\n",
    "\n",
    "            if update_stable:\n",
    "                # If policy isn't updated, stop iteration\n",
    "                break\n",
    "\n",
    "        # Turn dictionary to grid\n",
    "        V_grid = self.dict_to_grid(V)\n",
    "        return V_grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
